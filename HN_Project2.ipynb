{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv file...\n",
      "Getting Traning Data...\n",
      "Getting Testing Data...\n",
      "Reading stop files\n",
      "Smoothing value  0.1\n",
      "Building Model with smoothing =  0.1\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.2\n",
      "Building Model with smoothing =  0.2\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.3\n",
      "Building Model with smoothing =  0.3\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.4\n",
      "Building Model with smoothing =  0.4\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.5\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.6\n",
      "Building Model with smoothing =  0.6\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.7\n",
      "Building Model with smoothing =  0.7\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.8\n",
      "Building Model with smoothing =  0.8\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  0.9\n",
      "Building Model with smoothing =  0.9\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n",
      "Smoothing value  1.0\n",
      "Building Model with smoothing =  1.0\n",
      "Generate Vocabulary\n",
      "Returning list without removing stop words\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Returning list without removing stop words\n",
      "Generating model-2018.txt....\n",
      "Complete file  model-2018.txt\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseline-result.txt\n",
      "Calculating Smoothing performance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import io\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "class trainingData:\n",
    "    NUM_CLASSES = 0\n",
    "    VOCAB_LENGTH = 0\n",
    "    CLASS_TYPES = []\n",
    "    CLASS_VOCAB_LEN = []\n",
    "    CLASS_PROBABILITY = []\n",
    "    ORIGINAL_CLASSIFICATION = []\n",
    "    STOP_WORDS_VOCABULARY = []\n",
    "    PREDICTED_CLASSIFICATION = []\n",
    "    DICT_SCORE = {\"ACCURACY\": [], \"PRECISION\": [], \"RECALL\":\n",
    "        [], \"F1_SCORE\": []}\n",
    "\n",
    "\n",
    "\n",
    "def getVocabulary(trainData, stop_words, word_len_filter,freq_word_filter):\n",
    "    # Change all words to lowercase\n",
    "    newLowerTitle = trainData[\"Title\"].str.lower()\n",
    "    words = []\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    # Get Tokens\n",
    "    for index, row in newLowerTitle.iteritems():\n",
    "        words.append(tknzr.tokenize(row))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    vocabulary = []\n",
    "\n",
    "    # Lemmatize  words\n",
    "    for word in words:\n",
    "        for w in word:\n",
    "            vocabulary.append(lemmatizer.lemmatize(w))\n",
    "    # Remove Punctuation and special Characters\n",
    "    removePunVocabulary = [''.join(c for c in s if c not in string.punctuation) for s in vocabulary]\n",
    "    removePunVocabulary = [s for s in removePunVocabulary if s]\n",
    "    # newVocabulary = [re.sub('[^A-Za-z0-9]+', '', s) for s in removePunVocabulary]\n",
    "    removePunVocabulary.sort()\n",
    "    removePunVocabulary = list(filter(None, removePunVocabulary))\n",
    "\n",
    "    # Word Length filtering\n",
    "    if word_len_filter:\n",
    "        print('Word length filtering enabled')\n",
    "        removePunVocabulary = [word for word in removePunVocabulary if 2 < len(word) < 9]\n",
    "\n",
    "    # Remove stop words\n",
    "    if stop_words:\n",
    "        print(\"Stop words enabled\")\n",
    "        removed_stop_words_list = [word for word in removePunVocabulary if\n",
    "                                   word not in trainingData.STOP_WORDS_VOCABULARY]\n",
    "        return removed_stop_words_list\n",
    "    else:\n",
    "        print(\"Returning list without removing stop words\")\n",
    "        return removePunVocabulary\n",
    "\n",
    "\n",
    "def buildModel(trainData, smooth, stop_words, word_len_filt, freq_words_filter):\n",
    "    print(\"Building Model with smoothing = \", smooth)\n",
    "    # Generate Vocabulary\n",
    "    print(\"Generate Vocabulary\")\n",
    "    vocabulary = getVocabulary(trainData, stop_words, word_len_filt, freq_words_filter)\n",
    "    vocabulary = list(filter(None, vocabulary))\n",
    "    counter = collections.Counter(vocabulary)\n",
    "    vocabulary = list(counter.keys())\n",
    "    vocabulary = list(filter(None, vocabulary))\n",
    "    \n",
    "    '''\n",
    "     # Filtering words by freq\n",
    "    if freq_words_filter:\n",
    "        freq = 1\n",
    "        print('Removing words with Freq', freq)\n",
    "        fdist = FreqDist()\n",
    "        for word in vocabulary:\n",
    "            fdist[word] += 1\n",
    "        # For testing\n",
    "        words_freq_1 = list(filter(lambda x : x[1] == 1,fdist.items()))\n",
    "        print(words_freq_1)\n",
    "        print('Created Freq Dist')\n",
    "        print(fdist.items())\n",
    "        filterd_vocabulary = dict(filter(lambda x: x[1] > freq, fdist.items()))\n",
    "        vocabulary = [k for k in filterd_vocabulary]\n",
    "        print(vocabulary)\n",
    "    '''\n",
    "\n",
    "\n",
    "    # final sorted vocabulary\n",
    "    # vocabulary.sort()\n",
    "    with io.open('vocabulary.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in vocabulary:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    # Get length of vocabulary\n",
    "    vocabularyLen = len(vocabulary)\n",
    "    trainingData.VOCAB_LENGTH = vocabularyLen\n",
    "    print(\"Complete Vocabulary can be found in vocabulary.txt, vocabulary length = \", vocabularyLen)\n",
    "\n",
    "    # Get number of post types\n",
    "    print(\"Getting number of classes\")\n",
    "    post_type = trainData[\"Post Type\"]\n",
    "    count_type = collections.Counter(post_type)\n",
    "    types = list(count_type.keys())\n",
    "    trainingData.CLASS_TYPES = types\n",
    "    trainingData.NUM_CLASSES = len(types)\n",
    "    print(\"Classes: \", types)\n",
    "    classProbability(post_type, types)\n",
    "\n",
    "    counter = 0\n",
    "    # Get vocabulary set for each post type\n",
    "    print(\"Getting vocabulary of each class and frequency of words\")\n",
    "    for item in types:\n",
    "        sdata = \"Class\" + str(counter) + \"Data\"\n",
    "        globals()[sdata] = trainData[trainData[\"Post Type\"] == item]\n",
    "        svocab = \"Class\" + str(counter) + \"Vocab\"\n",
    "        globals()[svocab] = getVocabulary(globals()[sdata], stop_words, word_len_filt,freq_words_filter)\n",
    "        globals()[svocab] = list(filter(None, globals()[svocab]))\n",
    "        # length of vocabulary for eah post type\n",
    "        count_type = collections.Counter(globals()[svocab])\n",
    "        sLen = \"Class\" + str(counter) + \"Len\"\n",
    "        globals()[sLen] = len(list(count_type.keys()))\n",
    "        trainingData.CLASS_VOCAB_LEN.append(globals()[sLen])\n",
    "        # frequency of each word in each post type\n",
    "        sFreq = \"Class\" + str(counter) + \"Frequency\"\n",
    "        globals()[sFreq] = nltk.FreqDist(globals()[svocab])\n",
    "        counter = counter + 1\n",
    "\n",
    "\n",
    "\n",
    "    if stop_words:\n",
    "        print(\"Generating stopword-model.txt\")\n",
    "        file = io.open(\"stopword-model.txt\", \"w\", encoding=\"utf-8\")\n",
    "    elif word_len_filt:\n",
    "        print('Generating wordlength-model.txt')\n",
    "        file = io.open(\"wordlength-model.txt\", \"w\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        print(\"Generating model-2018.txt....\")\n",
    "        file = io.open(\"model-2018.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    line = 0\n",
    "    for word in vocabulary:\n",
    "        s = ''\n",
    "        line += 1\n",
    "        # print(line)\n",
    "        s += str(line) + \"  \"\n",
    "        s += word + \"  \"\n",
    "\n",
    "        for i in range(trainingData.NUM_CLASSES):\n",
    "            if globals()[\"Class\" + str(i) + \"Frequency\"].__contains__(word):\n",
    "                c = globals()[\"Class\" + str(i) + \"Frequency\"].get(word)\n",
    "            else:\n",
    "                c = 0\n",
    "            s += str(c) + \"  \"\n",
    "            itemLen = trainingData.CLASS_VOCAB_LEN[i]\n",
    "            probability = (c + smooth) / (itemLen + trainingData.VOCAB_LENGTH * smooth)\n",
    "            s += format(probability, '.12f') + \"  \"\n",
    "        s += '\\r'\n",
    "        file.write(s)\n",
    "    print(\"Complete file \", file.name)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def classProbability(trainClasses, post_types):\n",
    "    classFrequency = nltk.FreqDist(trainClasses)\n",
    "    totalPosts = len(trainClasses)\n",
    "    for item in post_types:\n",
    "        frequency = classFrequency.get(item)\n",
    "        class_prob = frequency / totalPosts\n",
    "        trainingData.CLASS_PROBABILITY.append(class_prob)\n",
    "\n",
    "\n",
    "def buildDictonary(file):\n",
    "    dictionary = {}\n",
    "    model_2018 = file.readlines()\n",
    "    model_2018 = [x.strip() for x in model_2018]\n",
    "    print(\"Building  dict\")\n",
    "\n",
    "    for row in model_2018:\n",
    "        row_split_list = row.split('  ')\n",
    "        dictionary[row_split_list[1]] = row_split_list[2:]\n",
    "\n",
    "    print(\"Created dict \")\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def sum_cond_prob(tokenList, dictionary, smooth, classNum):\n",
    "    sum = 0\n",
    "    # print(\"Calculate conditional probabiliy of words given Class\", classNum)\n",
    "    for token in tokenList:\n",
    "        if token in dictionary:\n",
    "            cond_prob = dictionary.get(token)\n",
    "            index = classNum + classNum + 1\n",
    "            prob = float(cond_prob[index])\n",
    "            sum = sum + math.log10(prob)\n",
    "        else:\n",
    "            prob = smooth / (trainingData.CLASS_VOCAB_LEN[classNum] + trainingData.VOCAB_LENGTH * smooth)\n",
    "            sum += math.log10(prob)\n",
    "    return sum\n",
    "\n",
    "\n",
    "def classify(tokenizeList, dictionary, smooth):\n",
    "    classifed_type = ''\n",
    "    scores = []\n",
    "    for i in range(trainingData.NUM_CLASSES):\n",
    "        probability = math.log10(trainingData.CLASS_PROBABILITY[i]) + sum_cond_prob(tokenizeList, dictionary, smooth, i)\n",
    "        scores.append(probability)\n",
    "    max_prob = max(scores)\n",
    "    for i in range(trainingData.NUM_CLASSES):\n",
    "        if max_prob == scores[i]:\n",
    "            # print(\"max_prob: \",max_prob)\n",
    "            classified_type = trainingData.CLASS_TYPES[i]\n",
    "    d = dict()\n",
    "    d['classType'] = classified_type\n",
    "    d['scores'] = scores\n",
    "    return d\n",
    "\n",
    "\n",
    "# Tokenize tile, Remove puncuations and lemmentize for each title\n",
    "def tokenize_title(title):\n",
    "    # print(\"Tokenizing test data title\")\n",
    "    tweetTokenizer = TweetTokenizer()\n",
    "    tokenList = tweetTokenizer.tokenize(title)\n",
    "    lemmentizeList = []\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    for word in tokenList:\n",
    "        lemmentizeList.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    removePunTokens = [''.join(c for c in s if c not in string.punctuation) for s in lemmentizeList]\n",
    "    removePunTokens = [s for s in removePunTokens if s]\n",
    "    # print(removePunTokens)\n",
    "    return removePunTokens\n",
    "\n",
    "\n",
    "def buildClassifier(testData, smoothing_value, stop_words, word_len_filter):\n",
    "    if stop_words:\n",
    "        file = io.open(\"stopword-model.txt\", encoding=\"utf-8\")\n",
    "        dictionary = buildDictonary(file)\n",
    "        print(\"Generating stopword-result.txt...\")\n",
    "        file = io.open(\"stopword-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    elif word_len_filter:\n",
    "        file = io.open(\"wordlength-model.txt\", encoding=\"utf-8\")\n",
    "        dictionary = buildDictonary(file)\n",
    "        print(\"Generating wordlength-result.txt...\")\n",
    "        file = io.open(\"wordlength-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        file = io.open(\"model-2018.txt\", encoding=\"utf-8\")\n",
    "        dictionary = buildDictonary(file)\n",
    "        print(\"Generating baseline-result.txt....\")\n",
    "        file = io.open(\"baseline-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    line = 0\n",
    "    for index, row in testData.iterrows():\n",
    "        label = \"right\";\n",
    "        s = ''\n",
    "        line += 1\n",
    "        s += str(line) + \"  \"\n",
    "        s += row[\"Title\"] + \"  \"\n",
    "        tokenizeList = tokenize_title(row[\"Title\"])\n",
    "        original_post_type = row[\"Post Type\"]\n",
    "        trainingData.ORIGINAL_CLASSIFICATION.append(original_post_type)\n",
    "        # print(\"Reading the contents of the model file\")\n",
    "        dList = classify(tokenizeList, dictionary, smoothing_value)\n",
    "        classification = dList.get('classType')\n",
    "        trainingData.PREDICTED_CLASSIFICATION.append(classification)\n",
    "        class_prob = dList.get('scores')\n",
    "        # print(line, \"original: \", original_post_type, \"classifier: \",classification)\n",
    "        if classification != original_post_type:\n",
    "            label = \"wrong\"\n",
    "        s += classification + \"  \"\n",
    "        for i in range(trainingData.NUM_CLASSES):\n",
    "            s += str(class_prob[i]) + \"  \"\n",
    "        s += label + \"  \"\n",
    "        s += '\\r'\n",
    "        file.write(s)\n",
    "    print(\"Completed\", file.name)\n",
    "\n",
    "\n",
    "def calculatebaseperformance():\n",
    "    target_names = trainingData.CLASS_TYPES\n",
    "    classification_report = metrics.classification_report(trainingData.ORIGINAL_CLASSIFICATION, trainingData.\n",
    "                                                          PREDICTED_CLASSIFICATION, target_names)\n",
    "    f1_score = metrics.f1_score(trainingData.PREDICTED_CLASSIFICATION, trainingData.ORIGINAL_CLASSIFICATION,\n",
    "                                target_names, average='weighted')\n",
    "    precision = metrics.precision_score(trainingData.PREDICTED_CLASSIFICATION, trainingData.ORIGINAL_CLASSIFICATION,\n",
    "                                        target_names, average='weighted')\n",
    "    print(\"F1 Score\", f1_score)\n",
    "    print(\"Precision\", precision)\n",
    "    print(metrics.classification_report(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                        target_names))\n",
    "\n",
    "def calculateSmoothingPerformance(smoothingvalue):\n",
    "    print(\"Calculating Smoothing performance\")\n",
    "    accuracy_score = metrics.accuracy_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                            trainingData.CLASS_TYPES)\n",
    "    precision = metrics.precision_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                        trainingData.CLASS_TYPES, average='weighted')\n",
    "    recall = metrics.recall_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                  trainingData.CLASS_TYPES, average='weighted')\n",
    "    f1_score = metrics.f1_score(trainingData.ORIGINAL_CLASSIFICATION, trainingData.PREDICTED_CLASSIFICATION,\n",
    "                                trainingData.CLASS_TYPES, average='weighted')\n",
    "    accuracy_list = trainingData.DICT_SCORE.get(\"ACCURACY\")\n",
    "    precision_list = trainingData.DICT_SCORE.get(\"PRECISION\")\n",
    "    recall_list = trainingData.DICT_SCORE.get(\"RECALL\")\n",
    "    f1_score_list = trainingData.DICT_SCORE.get(\"F1_SCORE\")\n",
    "\n",
    "    accuracy_list.append(round(accuracy_score, 3))\n",
    "    precision_list.append(round(precision, 3))\n",
    "    recall_list.append(round(recall, 3))\n",
    "    f1_score_list.append(round(f1_score, 3))\n",
    "    \n",
    "\n",
    "\n",
    "def plotPerformace():\n",
    "    smoothing_list = [x / 10 for x in range(1, 11)]\n",
    "    accuracy_list = trainingData.DICT_SCORE.get(\"ACCURACY\")\n",
    "    precision_list = trainingData.DICT_SCORE.get(\"PRECISION\")\n",
    "    recall_list = trainingData.DICT_SCORE.get(\"RECALL\")\n",
    "    f1_score_list = trainingData.DICT_SCORE.get(\"F1_SCORE\")\n",
    "\n",
    "    df = pd.DataFrame(np.c_[accuracy_list, precision_list, recall_list, f1_score_list], index=smoothing_list)\n",
    "    ax = df.plot.bar()\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    ax.legend([\"ACCURACY\", \"PRECISION\", \"RECALL\", \"F1_SCORE\"])\n",
    "    ax.set_xlabel(\"Smoothing values\")\n",
    "    ax.set_ylabel(\"Performance \")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read csv File\n",
    "    print(\"Reading csv file...\")\n",
    "    dataFrame = pd.read_csv(\"hn2018_2019.csv\")\n",
    "    dataFrame[\"Created At\"] = pd.to_datetime(dataFrame[\"Created At\"])\n",
    "    dataFrame[\"year\"] = dataFrame[\"Created At\"].dt.year\n",
    "    dataFrame[\"Title\"] = dataFrame[\"Title\"].str.lower()\n",
    "    # Get data frames from 2018 and 2019\n",
    "    print(\"Getting Traning Data...\")\n",
    "    trainData = dataFrame[dataFrame[\"year\"] == 2018]\n",
    "    print(\"Getting Testing Data...\")\n",
    "    testData = dataFrame[dataFrame[\"year\"] == 2019]\n",
    "    # Reading stop words\n",
    "    print('Reading stop files')\n",
    "    file = open('StopWords.txt', 'r')\n",
    "    trainingData.STOP_WORDS_VOCABULARY = file.read().splitlines()\n",
    "    stop_words = False\n",
    "    word_len_filt = False\n",
    "    freq_words_filter = False\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    smoothing_value = 0.5\n",
    "\n",
    "    # Baseline Results\n",
    "    buildModel(trainData, smoothing_value, stop_words, word_len_filt, freq_words_filter)\n",
    "    print(\"Taining complete...\")\n",
    "    print(\"Start testing...\")\n",
    "    startTime = time.time()\n",
    "    buildClassifier(testData, smoothing_value, stop_words, word_len_filt)\n",
    "    calculatebaseperformance()\n",
    "    stopTime = time.time()\n",
    "    runtime = stopTime - startTime\n",
    "    print(\"Runtime: \", runtime)\n",
    "    '''\n",
    "\n",
    "    # Experiment 5 Smoothing\n",
    "    smoothing_value_range = [x / 10 for x in range(1, 11)]\n",
    "    for smoothingvalue in smoothing_value_range:\n",
    "        print('Smoothing value ', smoothingvalue)\n",
    "        buildModel(trainData, smoothingvalue, stop_words,word_len_filt,freq_words_filter)\n",
    "        buildClassifier(testData, smoothingvalue, stop_words,word_len_filt)\n",
    "        calculateSmoothingPerformance(smoothingvalue)\n",
    "    plotPerformace()\n",
    " \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
