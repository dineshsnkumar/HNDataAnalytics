{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv file...\n",
      "Getting Traning Data...\n",
      "Getting Testing Data...\n",
      "Building Model with smoothing =  0.5\n",
      "Generate Vocabulary\n",
      "Complete Vocabulary can be found in vocabulary.txt, vocabulary length =  85305\n",
      "Getting number of classes\n",
      "Classes:  ['story', 'ask_hn', 'show_hn', 'poll']\n",
      "Getting vocabulary of each class and frequency of words\n",
      "Generating model-2018.txt....\n",
      "Complete file model-2018.txt\n",
      "Taining complete...\n",
      "Start testing...\n",
      "Building  dict\n",
      "Created dict \n",
      "Generating baseline-result.txt....\n",
      "Completed baseLine-result.txt\n",
      "Runtime:  79.53285837173462\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import io\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "class trainingData:\n",
    "    NUM_CLASSES = 0\n",
    "    VOCAB_LENGTH = 0\n",
    "    CLASS_TYPES = []\n",
    "    CLASS_VOCAB_LEN = []\n",
    "    CLASS_PROBABILITY = []\n",
    "\n",
    "\n",
    "def getVocabulary(trainData):\n",
    "    #Change all words to lowercase\n",
    "    newLowerTitle =trainData[\"Title\"].str.lower()\n",
    "    words = []\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    #Get Tokens\n",
    "    for index, row in newLowerTitle.iteritems():\n",
    "        words.append(tknzr.tokenize(row))\n",
    "    lemmatizer =nltk.stem.WordNetLemmatizer()\n",
    "    vocabulary =[]\n",
    "    \n",
    "    #Lemmatize  words\n",
    "    for word in words:\n",
    "        for w in word:\n",
    "            vocabulary.append(lemmatizer.lemmatize(w))  \n",
    "    #Remove Punctuation and special Characters\n",
    "    removePunVocabulary =[''.join(c for c in s if c not in string.punctuation) for s in vocabulary]\n",
    "    removePunVocabulary = [s for s in removePunVocabulary if s]\n",
    "    #newVocabulary = [re.sub('[^A-Za-z0-9]+', '', s) for s in removePunVocabulary]\n",
    "    removePunVocabulary.sort()\n",
    "    removePunVocabulary = list(filter(None, removePunVocabulary))\n",
    "    return removePunVocabulary\n",
    "\n",
    "def buildModel(trainData, smooth):\n",
    "    \n",
    "    print(\"Building Model with smoothing = \", smooth)\n",
    "    #Generate Vocabulary\n",
    "    print(\"Generate Vocabulary\")\n",
    "    vocabulary = getVocabulary(trainData)\n",
    "    vocabulary = list(filter(None, vocabulary))\n",
    "    counter = collections.Counter(vocabulary)\n",
    "    vocabulary = list(counter.keys())\n",
    "    vocabulary = list(filter(None, vocabulary))\n",
    "    #final sorted vocabulary\n",
    "    #vocabulary.sort()\n",
    "    with io.open('vocabulary.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in vocabulary:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    #Get length of vocabulary\n",
    "    vocabularyLen = len(vocabulary)\n",
    "    trainingData.VOCAB_LENGTH = vocabularyLen\n",
    "    print(\"Complete Vocabulary can be found in vocabulary.txt, vocabulary length = \", vocabularyLen)\n",
    "    \n",
    "    #Get number of post types\n",
    "    print(\"Getting number of classes\")\n",
    "    post_type = trainData[\"Post Type\"]\n",
    "    count_type = collections.Counter(post_type)\n",
    "    types = list(count_type.keys())\n",
    "    trainingData.CLASS_TYPES = types\n",
    "    trainingData.NUM_CLASSES = len(types)\n",
    "    print(\"Classes: \", types)\n",
    "    classProbability(post_type, types)\n",
    "    \n",
    "    counter = 0\n",
    "    #Get vocabulary set for each post type\n",
    "    print(\"Getting vocabulary of each class and frequency of words\" )\n",
    "    for item in types:\n",
    "        sdata =\"Class\"+str(counter)+\"Data\"\n",
    "        globals()[sdata] = trainData[trainData[\"Post Type\"] == item]\n",
    "        svocab =\"Class\"+str(counter)+\"Vocab\"\n",
    "        globals()[svocab] = getVocabulary(globals()[sdata])\n",
    "        globals()[svocab] = list(filter(None, globals()[svocab]))\n",
    "        #length of vocabulary for eah post type\n",
    "        count_type = collections.Counter(globals()[svocab])\n",
    "        sLen = \"Class\"+str(counter)+\"Len\"\n",
    "        globals()[sLen] = len(list(count_type.keys()))\n",
    "        trainingData.CLASS_VOCAB_LEN.append(globals()[sLen])\n",
    "        #frequency of each word in each post type\n",
    "        sFreq = \"Class\"+str(counter)+\"Frequency\"\n",
    "        globals()[sFreq] = nltk.FreqDist(globals()[svocab])\n",
    "        counter = counter+1\n",
    "    #Generate model-2018.txt\n",
    "   \n",
    "    print(\"Generating model-2018.txt....\")\n",
    "    file = io.open(\"model-2018.txt\", \"w\", encoding=\"utf-8\")\n",
    "    line = 0\n",
    "    for word in vocabulary:\n",
    "        s = ''\n",
    "        line +=1\n",
    "        #print(line)\n",
    "        s += str(line) + \"  \"\n",
    "        s += word + \"  \"\n",
    "        \n",
    "        for i in range(trainingData.NUM_CLASSES):\n",
    "            if globals()[\"Class\"+str(i)+\"Frequency\"].__contains__(word):\n",
    "                c = globals()[\"Class\"+str(i)+\"Frequency\"].get(word)\n",
    "            else:\n",
    "                c = 0\n",
    "            s += str(c) + \"  \"\n",
    "            itemLen = trainingData.CLASS_VOCAB_LEN[i]\n",
    "            probability = (c+smooth)/(itemLen + trainingData.VOCAB_LENGTH*smooth)\n",
    "            s += format(probability, '.12f') + \"  \"\n",
    "        s += '\\r'\n",
    "        file.write(s)\n",
    "        \n",
    "    print(\"Complete file model-2018.txt\")\n",
    "    file.close()\n",
    "\n",
    "def classProbability(trainClasses, post_types):\n",
    "    classFrequency = nltk.FreqDist(trainClasses)\n",
    "    totalPosts = len(trainClasses)\n",
    "    for item in post_types:\n",
    "        frequency = classFrequency.get(item)\n",
    "        class_prob = frequency/totalPosts\n",
    "        trainingData.CLASS_PROBABILITY.append(class_prob)\n",
    "    \n",
    "def buildDictonary(file):\n",
    "    dictionary = {}\n",
    "    model_2018 = file.readlines()\n",
    "    model_2018 = [x.strip() for x in model_2018]\n",
    "    print(\"Building  dict\")\n",
    "\n",
    "    for row in model_2018:\n",
    "        row_split_list = row.split('  ')\n",
    "        dictionary[row_split_list[1]] = row_split_list[2:]\n",
    "\n",
    "    print(\"Created dict \")\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def sum_cond_prob(tokenList, dictionary, smooth, classNum):\n",
    "    sum = 0\n",
    "    #print(\"Calculate conditional probabiliy of words given Class\", classNum)\n",
    "    for token in tokenList:\n",
    "        if token in dictionary:\n",
    "            cond_prob = dictionary.get(token)\n",
    "            index = classNum+classNum+1\n",
    "            prob = float(cond_prob[index])\n",
    "            sum = sum + math.log10(prob)\n",
    "        else:\n",
    "            prob = smooth / (trainingData.CLASS_VOCAB_LEN[classNum] +  trainingData.VOCAB_LENGTH*smooth)\n",
    "            sum += math.log10(prob)\n",
    "    return sum\n",
    "\n",
    "\n",
    "def classify(tokenizeList, dictionary, smooth):\n",
    "    classifed_type=''\n",
    "    scores = []\n",
    "    for i in range(trainingData.NUM_CLASSES):\n",
    "        probability = math.log10(trainingData.CLASS_PROBABILITY[i]) + sum_cond_prob(tokenizeList, dictionary, smooth, i)\n",
    "        scores.append(probability)\n",
    "    max_prob = max(scores)\n",
    "    for i in range(trainingData.NUM_CLASSES):\n",
    "        if max_prob == scores[i]:\n",
    "            #print(\"max_prob: \",max_prob)\n",
    "            classified_type = trainingData.CLASS_TYPES[i]\n",
    "    d =dict()\n",
    "    d['classType'] = classified_type\n",
    "    d['scores'] = scores\n",
    "    return d\n",
    "\n",
    "# Tokenize tile, Remove puncuations and lemmentize for each title\n",
    "def tokenize_title(title):\n",
    "    #print(\"Tokenizing test data title\")\n",
    "    tweetTokenizer = TweetTokenizer()\n",
    "    tokenList = tweetTokenizer.tokenize(title)\n",
    "    lemmentizeList = []\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    for word in tokenList:\n",
    "        lemmentizeList.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    removePunTokens = [''.join(c for c in s if c not in string.punctuation) for s in lemmentizeList]\n",
    "    removePunTokens = [s for s in removePunTokens if s]\n",
    "    #print(removePunTokens)\n",
    "    return removePunTokens\n",
    "\n",
    "\n",
    "def buildClassifier(testData, smoothing_value):\n",
    "    \n",
    "    file = io.open(\"model-2018.txt\", encoding=\"utf-8\")\n",
    "    dictionary = buildDictonary(file)\n",
    "    print(\"Generating baseline-result.txt....\")\n",
    "    file = io.open(\"baseline-result.txt\", \"w\", encoding=\"utf-8\")\n",
    "    line=0\n",
    "    for index, row in testData.iterrows():\n",
    "        label=\"right\";\n",
    "        s = ''\n",
    "        line += 1\n",
    "        s+= str(line) + \"  \"\n",
    "        s+= row[\"Title\"]+ \"  \"\n",
    "        tokenizeList = tokenize_title(row[\"Title\"])\n",
    "        original_post_type = row[\"Post Type\"]\n",
    "        #print(\"Reading the contents of the model file\")\n",
    "        dList = classify(tokenizeList, dictionary, smoothing_value)\n",
    "        classification = dList.get('classType')\n",
    "        class_prob = dList.get('scores')\n",
    "        #print(line, \"original: \", original_post_type, \"classifier: \",classification)\n",
    "        if classification!=original_post_type:\n",
    "            label=\"wrong\"\n",
    "        s+=classification + \"  \"\n",
    "        for i in range(trainingData.NUM_CLASSES):\n",
    "            s += str(class_prob[i]) + \"  \"\n",
    "        s += label + \"  \"\n",
    "        s += '\\r'\n",
    "        file.write(s)\n",
    "    print(\"Completed baseLine-result.txt\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    #Read csv File\n",
    "    print(\"Reading csv file...\")\n",
    "    dataFrame = pd.read_csv(\"hn2018_2019.csv\")\n",
    "    dataFrame[\"Created At\"] = pd.to_datetime(dataFrame[\"Created At\"])\n",
    "    dataFrame[\"year\"] = dataFrame[\"Created At\"].dt.year\n",
    "    dataFrame[\"Title\"] = dataFrame[\"Title\"].str.lower()\n",
    "\n",
    "    # Get data frames from 2018 and 2019\n",
    "    print(\"Getting Traning Data...\")\n",
    "    trainData = dataFrame[dataFrame[\"year\"] == 2018]\n",
    "    print(\"Getting Testing Data...\")\n",
    "    testData = dataFrame[dataFrame[\"year\"] == 2019]\n",
    "    buildModel(trainData, 0.5)\n",
    "    print(\"Taining complete...\")\n",
    "    print(\"Start testing...\")\n",
    "    startTime = time.time()\n",
    "    buildClassifier(testData, 0.5)\n",
    "    stopTime = time.time()\n",
    "    runtime = stopTime-startTime\n",
    "    print(\"Runtime: \", runtime)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
